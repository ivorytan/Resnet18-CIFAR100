{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IhcgBundQpt3"
   },
   "source": [
    "**第零部分：库导入**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0w1ybN_vQnF2"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import csv\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "os7Gg9GnR3-B"
   },
   "source": [
    "**第一部分：resnet代码**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kOKibVLjRsZO"
   },
   "outputs": [],
   "source": [
    "'''ResNet18/34/50/101/152 in Pytorch.'''\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(in_planes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = conv3x3(3,64)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18(num_classes=10):\n",
    "    return ResNet(BasicBlock, [2,2,2,2], num_classes)\n",
    "\n",
    "def ResNet34(num_classes=10):\n",
    "    return ResNet(BasicBlock, [3,4,6,3], num_classes)\n",
    "\n",
    "def ResNet50(num_classes=10):\n",
    "    return ResNet(Bottleneck, [3,4,6,3], num_classes)\n",
    "\n",
    "def ResNet101(num_classes=10):\n",
    "    return ResNet(Bottleneck, [3,4,23,3], num_classes)\n",
    "\n",
    "def ResNet152(num_classes=10):\n",
    "    return ResNet(Bottleneck, [3,8,36,3], num_classes)\n",
    "\n",
    "def test_resnet():\n",
    "    net = ResNet50()\n",
    "    y = net(Variable(torch.randn(1,3,32,32)))\n",
    "    print(y.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hG8JM6AJSLiF"
   },
   "source": [
    "**第二部分：utils使用工具**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LCOZWB_mU88B"
   },
   "source": [
    "write_csv定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5feyJ5EvU2J7"
   },
   "outputs": [],
   "source": [
    "class CSVLogger:\n",
    "    def __init__(self, args, fieldnames, filename='log.csv'):\n",
    "        self.filename = filename\n",
    "        self.csv_file = open(filename, 'a')\n",
    "        writer = csv.writer(self.csv_file)\n",
    "        if not os.path.exists(filename):\n",
    "          # Write model configuration at top of csv\n",
    "          for arg in vars(args):\n",
    "            writer.writerow([arg, args[arg]])\n",
    "        writer.writerow([''])\n",
    "\n",
    "        self.writer = csv.DictWriter(self.csv_file, fieldnames=fieldnames)\n",
    "        self.writer.writeheader()\n",
    "\n",
    "        self.csv_file.flush()\n",
    "\n",
    "    def writerow(self, row):\n",
    "        self.writer.writerow(row)\n",
    "        self.csv_file.flush()\n",
    "\n",
    "    def close(self):\n",
    "        self.csv_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MpeUxlyXVfc-"
   },
   "source": [
    "cutmix处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CJjKN4lwZ4Xr"
   },
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xg5y1bEZZ7SZ"
   },
   "source": [
    "cutout处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cV1YzG_JaAkp"
   },
   "outputs": [],
   "source": [
    "class Cutout(object):\n",
    "    def __init__(self, n_holes, length):\n",
    "        self.n_holes = n_holes\n",
    "        self.length = length\n",
    "\n",
    "    def __call__(self, img):\n",
    "        h = img.size(1)\n",
    "        w = img.size(2)\n",
    "        mask = np.ones((h, w), np.float32)\n",
    "        for n in range(self.n_holes):\n",
    "            y = np.random.randint(h)\n",
    "            x = np.random.randint(w)\n",
    "            y1 = np.clip(y - self.length // 2, 0, h)\n",
    "            y2 = np.clip(y + self.length // 2, 0, h)\n",
    "            x1 = np.clip(x - self.length // 2, 0, w)\n",
    "            x2 = np.clip(x + self.length // 2, 0, w)\n",
    "            mask[y1: y2, x1: x2] = 0.\n",
    "        mask = torch.from_numpy(mask)\n",
    "        mask = mask.expand_as(img)\n",
    "        img = img * mask\n",
    "\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23mbfBhMabj1"
   },
   "source": [
    "mixup处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0LUKnjpVafrQ"
   },
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=1.0, use_cuda=True):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    if use_cuda:\n",
    "        index = torch.randperm(batch_size).cuda()\n",
    "    else:\n",
    "        index = torch.randperm(batch_size)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ub4tylYKax5d"
   },
   "source": [
    "**第三部分：训练和测试函数定义**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SGcJFEJDa7fu"
   },
   "outputs": [],
   "source": [
    "def train_cutout(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    xentropy_loss_avg = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    progress_bar = tqdm(train_loader)\n",
    "    for i, (images, labels) in enumerate(progress_bar):\n",
    "        progress_bar.set_description('Epoch ' + str(epoch))\n",
    "        \n",
    "        if args['cuda']:\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        model.zero_grad()\n",
    "        pred = model(images)\n",
    "\n",
    "        xentropy_loss = criterion(pred, labels)\n",
    "        xentropy_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        xentropy_loss_avg += xentropy_loss.item()\n",
    "\n",
    "        # 计算训练过程中的准确率\n",
    "        pred = torch.max(pred.data, 1)[1]\n",
    "        total += labels.size(0)\n",
    "        correct += (pred == labels.data).sum().item()\n",
    "        accuracy = correct / total\n",
    "\n",
    "        # 打印训练过程中的loss和acc\n",
    "        progress_bar.set_postfix(xentropy='%.3f' % (xentropy_loss_avg / (i + 1)), acc='%.3f' % accuracy)\n",
    "\n",
    "    return (xentropy_loss_avg / (i + 1)), accuracy\n",
    "\n",
    "\n",
    "def train_mixup(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    progress_bar = tqdm(train_loader)\n",
    "    for batch_idx, (inputs, targets) in enumerate(progress_bar):\n",
    "        progress_bar.set_description('Epoch ' + str(epoch))\n",
    "        if args['cuda']:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "        # mixup数据处理\n",
    "        inputs, targets_a, targets_b, lam = mixup_data(inputs, targets, args['alpha'], args['cuda'])\n",
    "        inputs, targets_a, targets_b = map(Variable, (inputs, targets_a, targets_b))\n",
    "        model.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (lam * predicted.eq(targets_a.data).cpu().sum().float() + (1 - lam) * predicted.eq(targets_b.data).cpu().sum().float())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        progress_bar.set_postfix(xentropy='%.3f' % (train_loss / (batch_idx + 1)), acc='%.3f' % (correct / total))\n",
    "\n",
    "    return (train_loss / (batch_idx + 1)), (correct / total).item() / 100\n",
    "\n",
    "\n",
    "def train_cutmix(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    progress_bar = tqdm(train_loader)\n",
    "    for batch_idx, (inputs, targets) in enumerate(progress_bar):\n",
    "        progress_bar.set_description('Epoch ' + str(epoch))\n",
    "        if args['cuda']:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "        r = np.random.rand(1)\n",
    "        if args['alpha'] > 0 and r < args['cutmix_prob']: # 使用cutmix的概率\n",
    "          # 生成mix的样本\n",
    "            lam = np.random.beta(args['alpha'], args['alpha'])\n",
    "            if args['cuda']:\n",
    "                rand_index = torch.randperm(inputs.size()[0]).cuda()\n",
    "            else:\n",
    "                rand_index = torch.randperm(inputs.size()[0])\n",
    "            target_a = targets\n",
    "            target_b = targets[rand_index]\n",
    "            bbx1, bby1, bbx2, bby2 = rand_bbox(inputs.size(), lam)\n",
    "            inputs[:, :, bbx1:bbx2, bby1:bby2] = inputs[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "            # 调整lambda以与像素比匹配\n",
    "            lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (inputs.size()[-1] * inputs.size()[-2]))\n",
    "            # 计算输出\n",
    "            output = model(inputs)\n",
    "            loss = mixup_criterion(criterion, output, target_a, target_b, lam)\n",
    "        else:\n",
    "            # 计算输出\n",
    "            output = model(inputs)\n",
    "            loss = criterion(output, targets)\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets.data).cpu().sum().float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        progress_bar.set_postfix(xentropy='%.3f' % (train_loss / (batch_idx + 1)),\n",
    "                acc='%.3f' % (correct / total))\n",
    "\n",
    "    return (train_loss / (batch_idx + 1)), (correct / total).item()\n",
    "\n",
    "# 测试函数\n",
    "def test():\n",
    "    model.eval()\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "    for images, labels in test_loader:\n",
    "        if args['cuda']:\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = model(images)\n",
    "\n",
    "        pred = torch.max(pred.data, 1)[1]\n",
    "        total += labels.size(0)\n",
    "        correct += (pred == labels).sum().item()\n",
    "\n",
    "    test_acc = correct / total\n",
    "    model.train()\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BNqs0dQsSZfP"
   },
   "source": [
    "**第四部分：main操作**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "baseline方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义参数\n",
    "args = {\"method\": 'baseline',  # ['baseline', 'cutmix', 'cutout', 'mixup']\n",
    "      \"dataset\": 'cifar100',  # ['cifar10', 'cifar100']\n",
    "      \"model\": 'resnet18',  # ['resnet18]\n",
    "      \"batch_size\" : 128,  # [32, 64, 128] 固定为128进行训练\n",
    "      \"epochs\" : 50,     # 固定为50进行训练\n",
    "      \"learning_rate\": 0.1, # \n",
    "      \"data_augmentation\": True, # 数据增强，默认为True\n",
    "      \"no_cuda\": False, # 是否使用GPU\n",
    "      \"seed\": 0, \n",
    "      \"n_holes\": 1,\n",
    "      \"length\": 16,\n",
    "      \"alpha\": 0.2,\n",
    "      \"cutmix_prob\": 0.1\n",
    "}\n",
    "\n",
    "if args['method'] == 'cutout':\n",
    "    train = train_cutout\n",
    "elif args['method'] == 'mixup':\n",
    "    train = train_mixup\n",
    "elif args['method'] == 'cutmix':\n",
    "    train = train_cutmix\n",
    "elif args['method'] == 'baseline':\n",
    "    train = train_cutout\n",
    "else:\n",
    "    raise Exception('unknown method: {}'.format(args['method']))\n",
    "\n",
    "args['cuda'] = not args['no_cuda'] and torch.cuda.is_available()\n",
    "cudnn.benchmark = True\n",
    "\n",
    "torch.manual_seed(args['seed'])\n",
    "if args['cuda']:\n",
    "    torch.cuda.manual_seed(args['seed'])\n",
    "\n",
    "# 归一化\n",
    "normalize = transforms.Normalize(mean=[x/255.0 for x in [125.3, 123.0, 113.9]],\n",
    "                  std=[x/255.0 for x in [63.0, 62.1, 66.7]])\n",
    "\n",
    "# 训练集预处理\n",
    "train_transform = transforms.Compose([])\n",
    "if args['data_augmentation']:\n",
    "    train_transform.transforms.append(transforms.RandomCrop(32, padding=4))\n",
    "    train_transform.transforms.append(transforms.RandomHorizontalFlip())\n",
    "train_transform.transforms.append(transforms.ToTensor())\n",
    "train_transform.transforms.append(normalize)\n",
    "if args['method'] == 'cutout':\n",
    "    train_transform.transforms.append(Cutout(n_holes=args['n_holes'], length=args['length']))\n",
    "\n",
    "# 测试集预处理\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize])\n",
    "\n",
    "# 数据集选择\n",
    "if args['dataset'] == 'cifar10':\n",
    "    num_classes = 10\n",
    "    train_dataset = datasets.CIFAR10(root='./data/',\n",
    "                    train=True,\n",
    "                    transform=train_transform,\n",
    "                    download=True)\n",
    "    test_dataset = datasets.CIFAR10(root='./data/',\n",
    "                   train=False,\n",
    "                   transform=test_transform,\n",
    "                   download=True)\n",
    "elif args['dataset'] == 'cifar100':\n",
    "    num_classes = 100\n",
    "    train_dataset = datasets.CIFAR100(root='./data/',\n",
    "                    train=True,\n",
    "                    transform=train_transform,\n",
    "                    download=True)\n",
    "    test_dataset = datasets.CIFAR100(root='./data/',\n",
    "                   train=False,\n",
    "                   transform=test_transform,\n",
    "                   download=True)\n",
    "else:\n",
    "    raise Exception('unknown dataset: {}'.format(args['dataset']))\n",
    "\n",
    "# 定义Data Loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                       batch_size=args['batch_size'],\n",
    "                       shuffle=True,\n",
    "                       pin_memory=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                      batch_size=args['batch_size'],\n",
    "                      shuffle=False,\n",
    "                      pin_memory=True)\n",
    "\n",
    "# 选择训练模型\n",
    "if args['model'] == 'resnet18':\n",
    "    model = ResNet18(num_classes=num_classes)\n",
    "else:\n",
    "    raise Exception('unknown model: {}'.format(args['model']))\n",
    "\n",
    "if args['cuda']:\n",
    "    model = model.cuda()\n",
    "\n",
    "# 定义损失函数\n",
    "if args['cuda']:\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "else:\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=args['learning_rate'],\n",
    "                momentum=0.9, nesterov=True, weight_decay=5e-4)\n",
    "\n",
    "# 定义学习率优化\n",
    "scheduler = MultiStepLR(optimizer, milestones=[20, 40], gamma=0.2)\n",
    "\n",
    "# 获取存储名\n",
    "test_id = args['dataset'] + '_' + args['model'] + '_' + args['method']\n",
    "if not args['data_augmentation']:\n",
    "    test_id += '_noaugment'\n",
    "\n",
    "# 数据储存到csv文件\n",
    "try:\n",
    "    os.makedirs('./runs')\n",
    "except:\n",
    "    pass\n",
    "filename = './runs/' + test_id + '.csv'\n",
    "csv_logger = CSVLogger(args=args, fieldnames=['epoch', 'train_loss', 'train_acc', 'test_acc']\n",
    "                       , filename=filename)\n",
    "\n",
    "# 训练模型过程\n",
    "writer = SummaryWriter('./runs/train') # 使用tensorboard进行可视化\n",
    "for epoch in range(1, args['epochs'] + 1):\n",
    "    train_loss, train_acc = train(epoch)\n",
    "    test_acc = test()\n",
    "    tqdm.write('test_acc: %.3f' % test_acc)\n",
    "    scheduler.step()\n",
    "    row = {'epoch': str(epoch), 'train_loss':str(train_loss), 'train_acc': str(train_acc), 'test_acc': str(test_acc)}\n",
    "    csv_logger.writerow(row)\n",
    "    writer.add_scalar('train_loss', train_loss, global_step=epoch)\n",
    "    writer.add_scalar('train_acc', train_acc, global_step=epoch)\n",
    "    writer.add_scalar('test_acc', test_acc, global_step=epoch)\n",
    "writer.close()\n",
    "\n",
    "# 保存模型\n",
    "try:\n",
    "    os.makedirs('./checkpoints')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "torch.save(model.state_dict(), './checkpoints/' + test_id + '.pth')\n",
    "\n",
    "csv_logger.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cutmix方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-y414fknScoq",
    "outputId": "3be6fd43-1a79-480c-f529-518f4566c6e3"
   },
   "outputs": [],
   "source": [
    "# 定义参数\n",
    "args = {\"method\": 'cutmix',  # ['baseline', 'cutmix', 'cutout', 'mixup']\n",
    "      \"dataset\": 'cifar100',  # ['cifar10', 'cifar100']\n",
    "      \"model\": 'resnet18',  # ['resnet18]\n",
    "      \"batch_size\" : 128,  # [32, 64, 128] 固定为128进行训练\n",
    "      \"epochs\" : 50,     # 固定为50进行训练\n",
    "      \"learning_rate\": 0.1, # \n",
    "      \"data_augmentation\": True, # 数据增强，默认为True\n",
    "      \"no_cuda\": False, # 是否使用GPU\n",
    "      \"seed\": 0, \n",
    "      \"n_holes\": 1,\n",
    "      \"length\": 16,\n",
    "      \"alpha\": 0.2,\n",
    "      \"cutmix_prob\": 0.1\n",
    "}\n",
    "\n",
    "if args['method'] == 'cutout':\n",
    "    train = train_cutout\n",
    "elif args['method'] == 'mixup':\n",
    "    train = train_mixup\n",
    "elif args['method'] == 'cutmix':\n",
    "    train = train_cutmix\n",
    "elif args['method'] == 'baseline':\n",
    "    train = train_cutout\n",
    "else:\n",
    "    raise Exception('unknown method: {}'.format(args['method']))\n",
    "\n",
    "args['cuda'] = not args['no_cuda'] and torch.cuda.is_available()\n",
    "cudnn.benchmark = True\n",
    "\n",
    "torch.manual_seed(args['seed'])\n",
    "if args['cuda']:\n",
    "    torch.cuda.manual_seed(args['seed'])\n",
    "\n",
    "# 归一化\n",
    "normalize = transforms.Normalize(mean=[x/255.0 for x in [125.3, 123.0, 113.9]],\n",
    "                  std=[x/255.0 for x in [63.0, 62.1, 66.7]])\n",
    "\n",
    "# 训练集预处理\n",
    "train_transform = transforms.Compose([])\n",
    "if args['data_augmentation']:\n",
    "    train_transform.transforms.append(transforms.RandomCrop(32, padding=4))\n",
    "    train_transform.transforms.append(transforms.RandomHorizontalFlip())\n",
    "train_transform.transforms.append(transforms.ToTensor())\n",
    "train_transform.transforms.append(normalize)\n",
    "if args['method'] == 'cutout':\n",
    "    train_transform.transforms.append(Cutout(n_holes=args['n_holes'], length=args['length']))\n",
    "\n",
    "# 测试集预处理\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize])\n",
    "\n",
    "# 数据集选择\n",
    "if args['dataset'] == 'cifar10':\n",
    "    num_classes = 10\n",
    "    train_dataset = datasets.CIFAR10(root='./data/',\n",
    "                    train=True,\n",
    "                    transform=train_transform,\n",
    "                    download=True)\n",
    "    test_dataset = datasets.CIFAR10(root='./data/',\n",
    "                   train=False,\n",
    "                   transform=test_transform,\n",
    "                   download=True)\n",
    "elif args['dataset'] == 'cifar100':\n",
    "    num_classes = 100\n",
    "    train_dataset = datasets.CIFAR100(root='./data/',\n",
    "                    train=True,\n",
    "                    transform=train_transform,\n",
    "                    download=True)\n",
    "    test_dataset = datasets.CIFAR100(root='./data/',\n",
    "                   train=False,\n",
    "                   transform=test_transform,\n",
    "                   download=True)\n",
    "else:\n",
    "    raise Exception('unknown dataset: {}'.format(args['dataset']))\n",
    "\n",
    "# 定义Data Loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                       batch_size=args['batch_size'],\n",
    "                       shuffle=True,\n",
    "                       pin_memory=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                      batch_size=args['batch_size'],\n",
    "                      shuffle=False,\n",
    "                      pin_memory=True)\n",
    "\n",
    "# 选择训练模型\n",
    "if args['model'] == 'resnet18':\n",
    "    model = ResNet18(num_classes=num_classes)\n",
    "else:\n",
    "    raise Exception('unknown model: {}'.format(args['model']))\n",
    "\n",
    "if args['cuda']:\n",
    "    model = model.cuda()\n",
    "\n",
    "# 定义损失函数\n",
    "if args['cuda']:\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "else:\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=args['learning_rate'],\n",
    "                momentum=0.9, nesterov=True, weight_decay=5e-4)\n",
    "\n",
    "# 定义学习率优化\n",
    "scheduler = MultiStepLR(optimizer, milestones=[20, 40], gamma=0.2)\n",
    "\n",
    "# 获取存储名\n",
    "test_id = args['dataset'] + '_' + args['model'] + '_' + args['method']\n",
    "if not args['data_augmentation']:\n",
    "    test_id += '_noaugment'\n",
    "\n",
    "# 数据储存到csv文件\n",
    "try:\n",
    "    os.makedirs('./runs')\n",
    "except:\n",
    "    pass\n",
    "filename = './runs/' + test_id + '.csv'\n",
    "csv_logger = CSVLogger(args=args, fieldnames=['epoch', 'train_loss', 'train_acc', 'test_acc']\n",
    "                       , filename=filename)\n",
    "\n",
    "# 训练模型过程\n",
    "writer = SummaryWriter('./runs/train_cutmix') # 使用tensorboard进行可视化\n",
    "for epoch in range(1, args['epochs'] + 1):\n",
    "    train_loss, train_acc = train(epoch)\n",
    "    test_acc = test()\n",
    "    tqdm.write('test_acc: %.3f' % test_acc)\n",
    "    scheduler.step()\n",
    "    row = {'epoch': str(epoch), 'train_loss':str(train_loss), 'train_acc': str(train_acc), 'test_acc': str(test_acc)}\n",
    "    csv_logger.writerow(row)\n",
    "    writer.add_scalar('train_loss', train_loss, global_step=epoch)\n",
    "    writer.add_scalar('train_acc', train_acc, global_step=epoch)\n",
    "    writer.add_scalar('test_acc', test_acc, global_step=epoch)\n",
    "writer.close()\n",
    "\n",
    "# 保存模型\n",
    "try:\n",
    "    os.makedirs('./checkpoints')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "torch.save(model.state_dict(), './checkpoints/' + test_id + '.pth')\n",
    "\n",
    "csv_logger.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cutout方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义参数\n",
    "args = {\"method\": 'cutout',  # ['baseline', 'cutmix', 'cutout', 'mixup']\n",
    "      \"dataset\": 'cifar100',  # ['cifar10', 'cifar100']\n",
    "      \"model\": 'resnet18',  # ['resnet18]\n",
    "      \"batch_size\" : 128,  # [32, 64, 128] 固定为128进行训练\n",
    "      \"epochs\" : 50,     # 固定为50进行训练\n",
    "      \"learning_rate\": 0.1, # \n",
    "      \"data_augmentation\": True, # 数据增强，默认为True\n",
    "      \"no_cuda\": False, # 是否使用GPU\n",
    "      \"seed\": 0, \n",
    "      \"n_holes\": 1,\n",
    "      \"length\": 16,\n",
    "      \"alpha\": 0.2,\n",
    "      \"cutmix_prob\": 0.1\n",
    "}\n",
    "\n",
    "if args['method'] == 'cutout':\n",
    "    train = train_cutout\n",
    "elif args['method'] == 'mixup':\n",
    "    train = train_mixup\n",
    "elif args['method'] == 'cutmix':\n",
    "    train = train_cutmix\n",
    "elif args['method'] == 'baseline':\n",
    "    train = train_cutout\n",
    "else:\n",
    "    raise Exception('unknown method: {}'.format(args['method']))\n",
    "\n",
    "args['cuda'] = not args['no_cuda'] and torch.cuda.is_available()\n",
    "cudnn.benchmark = True\n",
    "\n",
    "torch.manual_seed(args['seed'])\n",
    "if args['cuda']:\n",
    "    torch.cuda.manual_seed(args['seed'])\n",
    "\n",
    "# 归一化\n",
    "normalize = transforms.Normalize(mean=[x/255.0 for x in [125.3, 123.0, 113.9]],\n",
    "                  std=[x/255.0 for x in [63.0, 62.1, 66.7]])\n",
    "\n",
    "# 训练集预处理\n",
    "train_transform = transforms.Compose([])\n",
    "if args['data_augmentation']:\n",
    "    train_transform.transforms.append(transforms.RandomCrop(32, padding=4))\n",
    "    train_transform.transforms.append(transforms.RandomHorizontalFlip())\n",
    "train_transform.transforms.append(transforms.ToTensor())\n",
    "train_transform.transforms.append(normalize)\n",
    "if args['method'] == 'cutout':\n",
    "    train_transform.transforms.append(Cutout(n_holes=args['n_holes'], length=args['length']))\n",
    "\n",
    "# 测试集预处理\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize])\n",
    "\n",
    "# 数据集选择\n",
    "if args['dataset'] == 'cifar10':\n",
    "    num_classes = 10\n",
    "    train_dataset = datasets.CIFAR10(root='./data/',\n",
    "                    train=True,\n",
    "                    transform=train_transform,\n",
    "                    download=True)\n",
    "    test_dataset = datasets.CIFAR10(root='./data/',\n",
    "                   train=False,\n",
    "                   transform=test_transform,\n",
    "                   download=True)\n",
    "elif args['dataset'] == 'cifar100':\n",
    "    num_classes = 100\n",
    "    train_dataset = datasets.CIFAR100(root='./data/',\n",
    "                    train=True,\n",
    "                    transform=train_transform,\n",
    "                    download=True)\n",
    "    test_dataset = datasets.CIFAR100(root='./data/',\n",
    "                   train=False,\n",
    "                   transform=test_transform,\n",
    "                   download=True)\n",
    "else:\n",
    "    raise Exception('unknown dataset: {}'.format(args['dataset']))\n",
    "\n",
    "# 定义Data Loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                       batch_size=args['batch_size'],\n",
    "                       shuffle=True,\n",
    "                       pin_memory=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                      batch_size=args['batch_size'],\n",
    "                      shuffle=False,\n",
    "                      pin_memory=True)\n",
    "\n",
    "# 选择训练模型\n",
    "if args['model'] == 'resnet18':\n",
    "    model = ResNet18(num_classes=num_classes)\n",
    "else:\n",
    "    raise Exception('unknown model: {}'.format(args['model']))\n",
    "\n",
    "if args['cuda']:\n",
    "    model = model.cuda()\n",
    "\n",
    "# 定义损失函数\n",
    "if args['cuda']:\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "else:\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=args['learning_rate'],\n",
    "                momentum=0.9, nesterov=True, weight_decay=5e-4)\n",
    "\n",
    "# 定义学习率优化\n",
    "scheduler = MultiStepLR(optimizer, milestones=[20, 40], gamma=0.2)\n",
    "\n",
    "# 获取存储名\n",
    "test_id = args['dataset'] + '_' + args['model'] + '_' + args['method']\n",
    "if not args['data_augmentation']:\n",
    "    test_id += '_noaugment'\n",
    "\n",
    "# 数据储存到csv文件\n",
    "try:\n",
    "    os.makedirs('./runs')\n",
    "except:\n",
    "    pass\n",
    "filename = './runs/' + test_id + '.csv'\n",
    "csv_logger = CSVLogger(args=args, fieldnames=['epoch', 'train_loss', 'train_acc', 'test_acc']\n",
    "                       , filename=filename)\n",
    "\n",
    "# 训练模型过程\n",
    "writer = SummaryWriter('./runs/train_cutout') # 使用tensorboard进行可视化\n",
    "for epoch in range(1, args['epochs'] + 1):\n",
    "    train_loss, train_acc = train(epoch)\n",
    "    test_acc = test()\n",
    "    tqdm.write('test_acc: %.3f' % test_acc)\n",
    "    scheduler.step()\n",
    "    row = {'epoch': str(epoch), 'train_loss':str(train_loss), 'train_acc': str(train_acc), 'test_acc': str(test_acc)}\n",
    "    csv_logger.writerow(row)\n",
    "    writer.add_scalar('train_loss', train_loss, global_step=epoch)\n",
    "    writer.add_scalar('train_acc', train_acc, global_step=epoch)\n",
    "    writer.add_scalar('test_acc', test_acc, global_step=epoch)\n",
    "writer.close()\n",
    "\n",
    "# 保存模型\n",
    "try:\n",
    "    os.makedirs('./checkpoints')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "torch.save(model.state_dict(), './checkpoints/' + test_id + '.pth')\n",
    "\n",
    "csv_logger.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mixup方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义参数\n",
    "args = {\"method\": 'mixup',  # ['baseline', 'cutmix', 'cutout', 'mixup']\n",
    "      \"dataset\": 'cifar100',  # ['cifar10', 'cifar100']\n",
    "      \"model\": 'resnet18',  # ['resnet18]\n",
    "      \"batch_size\" : 128,  # [32, 64, 128] 固定为128进行训练\n",
    "      \"epochs\" : 50,     # 固定为50进行训练\n",
    "      \"learning_rate\": 0.1, # \n",
    "      \"data_augmentation\": True, # 数据增强，默认为True\n",
    "      \"no_cuda\": False, # 是否使用GPU\n",
    "      \"seed\": 0, \n",
    "      \"n_holes\": 1,\n",
    "      \"length\": 16,\n",
    "      \"alpha\": 0.2,\n",
    "      \"cutmix_prob\": 0.1\n",
    "}\n",
    "\n",
    "if args['method'] == 'cutout':\n",
    "    train = train_cutout\n",
    "elif args['method'] == 'mixup':\n",
    "    train = train_mixup\n",
    "elif args['method'] == 'cutmix':\n",
    "    train = train_cutmix\n",
    "elif args['method'] == 'baseline':\n",
    "    train = train_cutout\n",
    "else:\n",
    "    raise Exception('unknown method: {}'.format(args['method']))\n",
    "\n",
    "args['cuda'] = not args['no_cuda'] and torch.cuda.is_available()\n",
    "cudnn.benchmark = True\n",
    "\n",
    "torch.manual_seed(args['seed'])\n",
    "if args['cuda']:\n",
    "    torch.cuda.manual_seed(args['seed'])\n",
    "\n",
    "# 归一化\n",
    "normalize = transforms.Normalize(mean=[x/255.0 for x in [125.3, 123.0, 113.9]],\n",
    "                  std=[x/255.0 for x in [63.0, 62.1, 66.7]])\n",
    "\n",
    "# 训练集预处理\n",
    "train_transform = transforms.Compose([])\n",
    "if args['data_augmentation']:\n",
    "    train_transform.transforms.append(transforms.RandomCrop(32, padding=4))\n",
    "    train_transform.transforms.append(transforms.RandomHorizontalFlip())\n",
    "train_transform.transforms.append(transforms.ToTensor())\n",
    "train_transform.transforms.append(normalize)\n",
    "if args['method'] == 'cutout':\n",
    "    train_transform.transforms.append(Cutout(n_holes=args['n_holes'], length=args['length']))\n",
    "\n",
    "# 测试集预处理\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize])\n",
    "\n",
    "# 数据集选择\n",
    "if args['dataset'] == 'cifar10':\n",
    "    num_classes = 10\n",
    "    train_dataset = datasets.CIFAR10(root='./data/',\n",
    "                    train=True,\n",
    "                    transform=train_transform,\n",
    "                    download=True)\n",
    "    test_dataset = datasets.CIFAR10(root='./data/',\n",
    "                   train=False,\n",
    "                   transform=test_transform,\n",
    "                   download=True)\n",
    "elif args['dataset'] == 'cifar100':\n",
    "    num_classes = 100\n",
    "    train_dataset = datasets.CIFAR100(root='./data/',\n",
    "                    train=True,\n",
    "                    transform=train_transform,\n",
    "                    download=True)\n",
    "    test_dataset = datasets.CIFAR100(root='./data/',\n",
    "                   train=False,\n",
    "                   transform=test_transform,\n",
    "                   download=True)\n",
    "else:\n",
    "    raise Exception('unknown dataset: {}'.format(args['dataset']))\n",
    "\n",
    "# 定义Data Loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                       batch_size=args['batch_size'],\n",
    "                       shuffle=True,\n",
    "                       pin_memory=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                      batch_size=args['batch_size'],\n",
    "                      shuffle=False,\n",
    "                      pin_memory=True)\n",
    "\n",
    "# 选择训练模型\n",
    "if args['model'] == 'resnet18':\n",
    "    model = ResNet18(num_classes=num_classes)\n",
    "else:\n",
    "    raise Exception('unknown model: {}'.format(args['model']))\n",
    "\n",
    "if args['cuda']:\n",
    "    model = model.cuda()\n",
    "\n",
    "# 定义损失函数\n",
    "if args['cuda']:\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "else:\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=args['learning_rate'],\n",
    "                momentum=0.9, nesterov=True, weight_decay=5e-4)\n",
    "\n",
    "# 定义学习率优化\n",
    "scheduler = MultiStepLR(optimizer, milestones=[20, 40], gamma=0.2)\n",
    "\n",
    "# 获取存储名\n",
    "test_id = args['dataset'] + '_' + args['model'] + '_' + args['method']\n",
    "if not args['data_augmentation']:\n",
    "    test_id += '_noaugment'\n",
    "\n",
    "# 数据储存到csv文件\n",
    "try:\n",
    "    os.makedirs('./runs')\n",
    "except:\n",
    "    pass\n",
    "filename = './runs/' + test_id + '.csv'\n",
    "csv_logger = CSVLogger(args=args, fieldnames=['epoch', 'train_loss', 'train_acc', 'test_acc']\n",
    "                       , filename=filename)\n",
    "\n",
    "# 训练模型过程\n",
    "writer = SummaryWriter('./runs/train_mixup') # 使用tensorboard进行可视化\n",
    "for epoch in range(1, args['epochs'] + 1):\n",
    "    train_loss, train_acc = train(epoch)\n",
    "    test_acc = test()\n",
    "    tqdm.write('test_acc: %.3f' % test_acc)\n",
    "    scheduler.step()\n",
    "    row = {'epoch': str(epoch), 'train_loss':str(train_loss), 'train_acc': str(train_acc), 'test_acc': str(test_acc)}\n",
    "    csv_logger.writerow(row)\n",
    "    writer.add_scalar('train_loss', train_loss, global_step=epoch)\n",
    "    writer.add_scalar('train_acc', train_acc, global_step=epoch)\n",
    "    writer.add_scalar('test_acc', test_acc, global_step=epoch)\n",
    "writer.close()\n",
    "\n",
    "# 保存模型\n",
    "try:\n",
    "    os.makedirs('./checkpoints')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "torch.save(model.state_dict(), './checkpoints/' + test_id + '.pth')\n",
    "\n",
    "csv_logger.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第五部分：进行图片显示**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils as utils\n",
    "\n",
    "print(\"获取3张图片\")\n",
    "imgs, labels = next(iter(train_loader))\n",
    "imgs=imgs[0:3]\n",
    "print(imgs.shape)\n",
    "\n",
    "# 经过数据增强后的原图\n",
    "images = utils.make_grid(imgs)\n",
    "images = images.numpy().transpose(1, 2, 0)\n",
    "plt.imshow(images)\n",
    "plt.show()\n",
    "\n",
    "# 经过cutout处理\n",
    "cut=Cutout(n_holes=1, length=16)\n",
    "imgs1=[]\n",
    "for i in range(3):\n",
    "    out=cut(imgs[i])\n",
    "    imgs1.append(out)\n",
    "images = utils.make_grid(imgs1)\n",
    "images = images.numpy().transpose(1, 2, 0)\n",
    "plt.imshow(images)\n",
    "plt.show()\n",
    "\n",
    "# 经过mixup处理\n",
    "lam = np.random.beta(args['alpha'], args['alpha'])\n",
    "batch_size = imgs.size()[0]\n",
    "index = torch.randperm(batch_size)\n",
    "imgs2 = lam * imgs + (1 - lam) * imgs[index, :]\n",
    "images = utils.make_grid(imgs2)\n",
    "images = images.numpy().transpose(1, 2, 0)\n",
    "plt.imshow(images)\n",
    "plt.show()\n",
    "\n",
    "# 经过cutmix处理\n",
    "r = np.random.rand(1)\n",
    "lam = np.random.beta(args['alpha'], args['alpha'])\n",
    "rand_index = torch.randperm(imgs.size()[0])\n",
    "# target_a = targets\n",
    "# target_b = targets[rand_index]\n",
    "bbx1, bby1, bbx2, bby2 = rand_bbox(imgs.size(), lam)\n",
    "imgs[:, :, bbx1:bbx2, bby1:bby2] = imgs[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "# 调整lambda以与像素比匹配\n",
    "lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (imgs.size()[-1] * imgs.size()[-2]))\n",
    "# imgs3, y_a, y_b, lam = cutmix.cutmix_data(imgs, labels)\n",
    "images = utils.make_grid(imgs)\n",
    "images = images.numpy().transpose(1, 2, 0)\n",
    "plt.imshow(images)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第六部分：模型加载和测试**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义参数\n",
    "args = {\"method\": 'mixup',  # ['baseline', 'cutmix', 'cutout', 'mixup']\n",
    "      \"dataset\": 'cifar100',  # ['cifar10', 'cifar100']\n",
    "      \"model\": 'resnet18',  # ['resnet18]\n",
    "      \"batch_size\" : 128,  # [32, 64, 128] 固定为128进行训练\n",
    "      \"epochs\" : 50,     # 固定为50进行训练\n",
    "      \"learning_rate\": 0.1, # \n",
    "      \"data_augmentation\": True, # 数据增强，默认为True\n",
    "      \"no_cuda\": False, # 是否使用GPU\n",
    "      \"seed\": 0, \n",
    "      \"n_holes\": 1,\n",
    "      \"length\": 16,\n",
    "      \"alpha\": 0.2,\n",
    "      \"cutmix_prob\": 0.1\n",
    "}\n",
    "\n",
    "if args['method'] == 'cutout':\n",
    "    train = train_cutout\n",
    "elif args['method'] == 'mixup':\n",
    "    train = train_mixup\n",
    "elif args['method'] == 'cutmix':\n",
    "    train = train_cutmix\n",
    "elif args['method'] == 'baseline':\n",
    "    train = train_cutout\n",
    "else:\n",
    "    raise Exception('unknown method: {}'.format(args['method']))\n",
    "\n",
    "args['cuda'] = not args['no_cuda'] and torch.cuda.is_available()\n",
    "cudnn.benchmark = True\n",
    "\n",
    "torch.manual_seed(args['seed'])\n",
    "if args['cuda']:\n",
    "    torch.cuda.manual_seed(args['seed'])\n",
    "\n",
    "# 归一化\n",
    "normalize = transforms.Normalize(mean=[x/255.0 for x in [125.3, 123.0, 113.9]],\n",
    "                  std=[x/255.0 for x in [63.0, 62.1, 66.7]])\n",
    "\n",
    "# 训练集预处理\n",
    "train_transform = transforms.Compose([])\n",
    "if args['data_augmentation']:\n",
    "    train_transform.transforms.append(transforms.RandomCrop(32, padding=4))\n",
    "    train_transform.transforms.append(transforms.RandomHorizontalFlip())\n",
    "train_transform.transforms.append(transforms.ToTensor())\n",
    "train_transform.transforms.append(normalize)\n",
    "if args['method'] == 'cutout':\n",
    "    train_transform.transforms.append(Cutout(n_holes=args['n_holes'], length=args['length']))\n",
    "\n",
    "# 测试集预处理\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize])\n",
    "\n",
    "# 数据集选择\n",
    "if args['dataset'] == 'cifar10':\n",
    "    num_classes = 10\n",
    "    train_dataset = datasets.CIFAR10(root='./data/',\n",
    "                    train=True,\n",
    "                    transform=train_transform,\n",
    "                    download=True)\n",
    "    test_dataset = datasets.CIFAR10(root='./data/',\n",
    "                   train=False,\n",
    "                   transform=test_transform,\n",
    "                   download=True)\n",
    "elif args['dataset'] == 'cifar100':\n",
    "    num_classes = 100\n",
    "    train_dataset = datasets.CIFAR100(root='./data/',\n",
    "                    train=True,\n",
    "                    transform=train_transform,\n",
    "                    download=True)\n",
    "    test_dataset = datasets.CIFAR100(root='./data/',\n",
    "                   train=False,\n",
    "                   transform=test_transform,\n",
    "                   download=True)\n",
    "else:\n",
    "    raise Exception('unknown dataset: {}'.format(args['dataset']))\n",
    "\n",
    "# 定义Data Loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                       batch_size=args['batch_size'],\n",
    "                       shuffle=True,\n",
    "                       pin_memory=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                      batch_size=args['batch_size'],\n",
    "                      shuffle=False,\n",
    "                      pin_memory=True)\n",
    "\n",
    "# 选择训练模型\n",
    "if args['model'] == 'resnet18':\n",
    "    model = ResNet18(num_classes=num_classes)\n",
    "else:\n",
    "    raise Exception('unknown model: {}'.format(args['model']))\n",
    "\n",
    "if args['cuda']:\n",
    "    model = model.cuda()\n",
    "    \n",
    "test_id = args['dataset'] + '_' + args['model'] + '_' + args['method']\n",
    "if not args['data_augmentation']:\n",
    "    test_id += '_noaugment'\n",
    "\n",
    "model.load_state_dict(torch.load('./checkpoints/' + test_id + '.pth'))\n",
    "\n",
    "# 测试\n",
    "test_acc = test()\n",
    "\n",
    "print(test_acc)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cifar100-resnet18.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
